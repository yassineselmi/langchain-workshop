{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yassineselmi/langchain-workshop/blob/main/lab/02_langchain_conversational_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hcqKO0aI6_PI",
      "metadata": {
        "id": "hcqKO0aI6_PI"
      },
      "source": [
        "# Conversational Memory\n",
        "\n",
        "Conversational memory is how chatbots can respond to our queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.\n",
        "\n",
        "The memory allows a _\"agent\"_ to remember previous interactions with the user. By default, agents are *stateless* — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.\n",
        "\n",
        "There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.\n",
        "\n",
        "In this notebook we'll explore this form of memory in the context of the LangChain library.\n",
        "\n",
        "We'll start by importing all of the libraries that we'll be using in this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "id": "uZR3iGJJtdDE"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_community langchain-openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66fb9c2a",
      "metadata": {
        "id": "66fb9c2a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain, ConversationChain\n",
        "from langchain.chains.conversation.memory import (ConversationBufferMemory,\n",
        "                                                  ConversationSummaryMemory,\n",
        "                                                  ConversationBufferWindowMemory,\n",
        "                                                  ConversationKGMemory)\n",
        "from langchain.callbacks import get_openai_callback\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c02c4fa2",
      "metadata": {
        "id": "c02c4fa2"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "baaa74b8",
      "metadata": {
        "id": "baaa74b8"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309g_2pqxzzB",
      "metadata": {
        "id": "309g_2pqxzzB"
      },
      "source": [
        "Later we will make use of a `count_tokens` utility function. This will allow us to count the number of tokens we are using for each call. We define it as so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "DsC3szr6yP3L",
      "metadata": {
        "id": "DsC3szr6yP3L"
      },
      "outputs": [],
      "source": [
        "def count_tokens(chain, query):\n",
        "    with get_openai_callback() as cb:\n",
        "        result = chain.invoke(query)\n",
        "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CnNF6i9r8RY_",
      "metadata": {
        "id": "CnNF6i9r8RY_"
      },
      "source": [
        "Now let's dive into **Conversational Memory**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e1f31b4",
      "metadata": {
        "id": "6e1f31b4"
      },
      "source": [
        "## What is memory?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b919c3a",
      "metadata": {
        "id": "5b919c3a"
      },
      "source": [
        "**Definition**: Memory is an agent's capacity of remembering previous interactions with the user (think chatbots)\n",
        "\n",
        "The official definition of memory is the following:\n",
        "\n",
        "\n",
        "> By default, Chains and Agents are stateless, meaning that they treat each incoming query independently. In some applications (chatbots being a GREAT example) it is highly important to remember previous interactions, both at a short term but also at a long term level. The concept of “Memory” exists to do exactly that.\n",
        "\n",
        "\n",
        "As we will see, although this sounds really straightforward there are several different ways to implement this memory capability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3343a0e2",
      "metadata": {
        "id": "3343a0e2"
      },
      "source": [
        "Before we delve into the different memory modules that the library offers, we will introduce the chain we will be using for these examples: the `ConversationChain`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c9c13e9",
      "metadata": {
        "id": "6c9c13e9"
      },
      "source": [
        "As always, when understanding a chain it is interesting to peek into its prompt first and then take a look at its `._call` method. As we saw in the chapter on chains, we can check out the prompt by accessing the `template` within the `prompt` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "96ff1ce3",
      "metadata": {
        "id": "96ff1ce3"
      },
      "outputs": [],
      "source": [
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ad394d",
      "metadata": {
        "id": "90ad394d"
      },
      "outputs": [],
      "source": [
        "print(conversation.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8b1e0c",
      "metadata": {
        "id": "9f8b1e0c"
      },
      "source": [
        "Interesting! So this chain's prompt is telling it to chat with the user and try to give truthful answers. If we look closely, there is a new component in the prompt that we didn't see when we were tinkering with the `LLMChain`: _history_. This is where our memory will come into play."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aaa70bf",
      "metadata": {
        "id": "6aaa70bf"
      },
      "source": [
        "So basically this chain combines an input from the user with the conversation history to generate a meaningful (and hopefully truthful) response."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19f5172f",
      "metadata": {
        "id": "19f5172f"
      },
      "source": [
        "Now that we've understood the basics of the chain we'll be using, we can get into memory. Let's dive in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f1a33f6",
      "metadata": {
        "id": "0f1a33f6"
      },
      "source": [
        "## Memory types"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d732b7a",
      "metadata": {
        "id": "4d732b7a"
      },
      "source": [
        "In this section we will review several memory types and analyze the pros and cons of each one, so you can choose the best one for your use case."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d70642",
      "metadata": {
        "id": "04d70642"
      },
      "source": [
        "### Memory type #1: ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d3cb2b",
      "metadata": {
        "id": "53d3cb2b"
      },
      "source": [
        "The `ConversationBufferMemory` does just what its name suggests: it keeps a buffer of the previous conversation excerpts as part of the context in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80a974a",
      "metadata": {
        "id": "d80a974a"
      },
      "source": [
        "**Key feature:** _the conversation buffer memory keeps the previous pieces of conversation completely unmodified, in their raw form._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2267f1f0",
      "metadata": {
        "id": "2267f1f0"
      },
      "outputs": [],
      "source": [
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferMemory()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lseziAMcAyvX",
      "metadata": {
        "id": "lseziAMcAyvX"
      },
      "source": [
        "We pass a user prompt the the `ConversationBufferMemory` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M0cwooC5A5Id",
      "metadata": {
        "id": "M0cwooC5A5Id"
      },
      "outputs": [],
      "source": [
        "conversation_buf.invoke(\"Good morning AI!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xlKINTFYA9eo",
      "metadata": {
        "id": "xlKINTFYA9eo"
      },
      "source": [
        "This one call used a total of `85` tokens, but we can't see that from the above. If we'd like to count the number of tokens being used we just pass our conversation chain object and the message we'd like to input via the `count_tokens` function we defined earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1bd5a88",
      "metadata": {
        "id": "d1bd5a88"
      },
      "outputs": [],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146170ca",
      "metadata": {
        "id": "146170ca"
      },
      "outputs": [],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e15411a",
      "metadata": {
        "id": "3e15411a"
      },
      "outputs": [],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3352cc48",
      "metadata": {
        "id": "3352cc48"
      },
      "outputs": [],
      "source": [
        "count_tokens(\n",
        "    conversation_buf,\n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "431b74ff",
      "metadata": {
        "id": "431b74ff"
      },
      "source": [
        "Our LLM with `ConversationBufferMemory` can clearly remember earlier interactions in the conversation. Let's take a closer look to how the LLM is saving our previous conversation. We can do this by accessing the `.buffer` attribute for the `.memory` in our chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "984afd09",
      "metadata": {
        "id": "984afd09"
      },
      "outputs": [],
      "source": [
        "print(conversation_buf.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4570267d",
      "metadata": {
        "id": "4570267d"
      },
      "source": [
        "Nice! So every piece of our conversation has been explicitly recorded and sent to the LLM in the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acf1a90b",
      "metadata": {
        "id": "acf1a90b"
      },
      "source": [
        "### Memory type #2: ConversationSummaryMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f61fe9",
      "metadata": {
        "id": "01f61fe9"
      },
      "source": [
        "The problem with the `ConversationBufferMemory` is that as the conversation progresses, the token count of our context history adds up. This is problematic because we might max out our LLM with a prompt that is too large to be processed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0516c7d4",
      "metadata": {
        "id": "0516c7d4"
      },
      "source": [
        "Enter `ConversationSummaryMemory`.\n",
        "\n",
        "Again, we can infer from the name what is going on.. we will keep a summary of our previous conversation snippets as our history. How will we summarize these? LLM to the rescue."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b0a905",
      "metadata": {
        "id": "86b0a905"
      },
      "source": [
        "**Key feature:** _the conversation summary memory keeps the previous pieces of conversation in a summarized form, where the summarization is performed by an LLM._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea6050c",
      "metadata": {
        "id": "0ea6050c"
      },
      "source": [
        "In this case we need to send the llm to our memory constructor to power its summarization ability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f33a16a7",
      "metadata": {
        "id": "f33a16a7"
      },
      "outputs": [],
      "source": [
        "conversation_sum = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationSummaryMemory(llm=llm)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b64c4896",
      "metadata": {
        "id": "b64c4896"
      },
      "source": [
        "When we have an llm, we always have a prompt ;) Let's see what's going on inside our conversation summary memory:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c476824d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c476824d",
        "outputId": "bf6147ab-7cdf-4c6a-a5f0-4537c8d631a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\n",
            "\n",
            "EXAMPLE\n",
            "Current summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\n",
            "\n",
            "New lines of conversation:\n",
            "Human: Why do you think artificial intelligence is a force for good?\n",
            "AI: Because artificial intelligence will help humans reach their full potential.\n",
            "\n",
            "New summary:\n",
            "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
            "END OF EXAMPLE\n",
            "\n",
            "Current summary:\n",
            "{summary}\n",
            "\n",
            "New lines of conversation:\n",
            "{new_lines}\n",
            "\n",
            "New summary:\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df90cdf3",
      "metadata": {
        "id": "df90cdf3"
      },
      "source": [
        "Cool! So each new interaction is summarized and appended to a running summary as the memory of our chain. Let's see how this works in practice!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "34343665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34343665",
        "outputId": "c9688269-ced3-40f9-9f04-1a2c18d1877e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 249 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# without count_tokens we'd call `conversation_sum(\"Good morning AI!\")`\n",
        "# but let's keep track of our tokens:\n",
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b757bba3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b757bba3",
        "outputId": "44532f72-5ce6-47a4-973d-19e1e294f0e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 380 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
              " 'history': 'The human greets the AI and the AI responds politely, asking how it can assist the human.',\n",
              " 'response': 'Hello! That sounds like an interesting topic. How can I assist you in exploring the potential of integrating Large Language Models with external knowledge?'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d0a373e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0a373e2",
        "outputId": "327334c8-5a98-414c-b16e-a14290764ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 814 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I just want to analyze the different possibilities. What can you think of?',\n",
              " 'history': 'The human greets the AI and the AI responds politely, asking how it can assist the human. The human expresses their interest in exploring the potential of integrating Large Language Models with external knowledge. The AI responds enthusiastically, offering its assistance in exploring this topic.',\n",
              " 'response': \"Hello! I'm here to assist you. When it comes to integrating Large Language Models with external knowledge, there are several possibilities to consider. One approach is to use pre-trained models like GPT-3 and fine-tune them on specific tasks or domains using external knowledge. This can help improve the model's performance and make it more specialized in certain areas.\\n\\nAnother possibility is to leverage existing knowledge graphs or databases to enhance the model's understanding and generate more accurate responses. By connecting the model to external sources of information, it can access a vast amount of structured data and use it to provide more contextually relevant answers.\\n\\nAdditionally, you can explore techniques like knowledge distillation, where you train a smaller model using a larger model's knowledge. This can help create more efficient models that still benefit from the external knowledge contained in the larger model.\\n\\nThese are just a few ideas to get started. Let me know if you'd like more information on any specific approach or if you have any other questions!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "2e286f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e286f0d",
        "outputId": "0a8c0e79-f8ce-443a-8119-bb8e0cd93860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 1027 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which data source types could be used to give context to the model?',\n",
              " 'history': 'The human greets the AI and the AI responds politely, offering its assistance. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI enthusiastically provides several possibilities, including fine-tuning pre-trained models, leveraging existing knowledge graphs or databases, and using knowledge distillation to create more efficient models. The AI offers to provide more information or answer any other questions the human may have.',\n",
              " 'response': \"There are several data source types that can be used to give context to a model. One option is to leverage existing knowledge graphs or databases, such as Wikidata or DBpedia, which contain structured information about various topics. These knowledge graphs can provide valuable context and facts that can be incorporated into the model's understanding.\\n\\nAnother option is to use web scraping techniques to extract information from websites or online sources. This can be particularly useful for obtaining up-to-date information or specific domain knowledge that may not be available in existing knowledge graphs.\\n\\nAdditionally, domain-specific datasets can be used to provide context to the model. For example, in the medical domain, there are publicly available datasets like MIMIC-III or PubMed that contain a wealth of information that can be used to enhance the model's understanding of medical concepts.\\n\\nIt's also worth mentioning that fine-tuning pre-trained models on domain-specific datasets can help the model gain context and domain expertise. By training the model on data relevant to a specific domain, it can learn to generate more accurate and contextually appropriate responses.\\n\\nThese are just a few examples, but there are many other data source types that can be explored depending on the specific requirements and goals of the integration. Let me know if you'd like more information on any of these options or if you have any other questions!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "891180f2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "891180f2",
        "outputId": "c1ff675a-8e3f-42f8-9b6f-6dc40fc24781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 586 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my aim again?',\n",
              " 'history': \"The human greets the AI and the AI responds politely, offering its assistance. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI enthusiastically provides several possibilities, including fine-tuning pre-trained models, leveraging existing knowledge graphs or databases, using web scraping techniques, and utilizing domain-specific datasets. The AI mentions that these data source types can provide valuable context and facts to enhance the model's understanding. The AI offers to provide more information or answer any other questions the human may have.\",\n",
              " 'response': 'Your aim is to explore the potential of integrating Large Language Models with external knowledge.'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_sum,\n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "2d768e44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d768e44",
        "outputId": "8c5975a4-53d4-4981-e51d-d33b05d59a40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The human greets the AI and the AI responds politely, offering its assistance. The human expresses interest in exploring the potential of integrating Large Language Models with external knowledge. The AI enthusiastically provides several possibilities, including fine-tuning pre-trained models, leveraging existing knowledge graphs or databases, using web scraping techniques, and utilizing domain-specific datasets. The AI mentions that these data source types can provide valuable context and facts to enhance the model's understanding. The AI offers to provide more information or answer any other questions the human may have. The human asks what their aim is, and the AI responds that their aim is to explore the potential of integrating Large Language Models with external knowledge.\n"
          ]
        }
      ],
      "source": [
        "print(conversation_sum.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd35c8c",
      "metadata": {
        "id": "0dd35c8c"
      },
      "source": [
        "You might be wondering.. if the aggregate token count is greater in each call here than in the buffer example, why should we use this type of memory? Well, if we check out buffer we will realize that although we are using more tokens in each instance of our conversation, our final history is shorter. This will enable us to have many more interactions before we reach our prompt's max length, making our chatbot more robust to longer conversations.\n",
        "\n",
        "We can count the number of tokens being used (without making a call to OpenAI) using the `tiktoken` tokenizer like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "nzijj4RZFX3I",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzijj4RZFX3I",
        "outputId": "78d4a8f9-efa8-4230-8da9-221d4985aaa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer memory conversation length: 1009\n",
            "Summary memory conversation length: 133\n"
          ]
        }
      ],
      "source": [
        "# initialize tokenizer\n",
        "tokenizer = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
        "\n",
        "# show number of tokens for the memory used by each memory type\n",
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bab0c09",
      "metadata": {
        "id": "2bab0c09"
      },
      "source": [
        "_Practical Note: the `text-davinci-003` and `gpt-3.5-turbo` models [have](https://platform.openai.com/docs/api-reference/completions/create#completions/create-max_tokens) a large max tokens count of 4096 tokens between prompt and answer._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "494830ea",
      "metadata": {
        "id": "494830ea"
      },
      "source": [
        "### Memory type #3: ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00762844",
      "metadata": {
        "id": "00762844"
      },
      "source": [
        "Another great option for these cases is the `ConversationBufferWindowMemory` where we will be keeping a few of the last interactions in our memory but we will intentionally drop the oldest ones - short-term memory if you'd like. Here the aggregate token count **and** the per-call token count will drop noticeably. We will control this window with the `k` parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "206a5915",
      "metadata": {
        "id": "206a5915"
      },
      "source": [
        "**Key feature:** _the conversation buffer window memory keeps the latest pieces of the conversation in raw form_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "45be373a",
      "metadata": {
        "id": "45be373a"
      },
      "outputs": [],
      "source": [
        "conversation_bufw = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationBufferWindowMemory(k=1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "fc4dd8a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc4dd8a0",
        "outputId": "9d0f096d-98f2-4a1e-db06-1159a87b2af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 77 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Good morning AI!',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How can I assist you today?'}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Good morning AI!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b9992e8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9992e8d",
        "outputId": "002d5e53-eec9-4231-a3b2-f3959c31058f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 374 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My interest here is to explore the potential of integrating Large Language Models with external knowledge',\n",
              " 'history': 'Human: Good morning AI!\\nAI: Good morning! How can I assist you today?',\n",
              " 'response': \"That's a fascinating topic! Integrating Large Language Models with external knowledge can greatly enhance their capabilities and make them more useful in various applications. By incorporating external knowledge sources, such as databases, knowledge graphs, or even the internet, language models can access a vast amount of information to provide more accurate and contextually relevant responses.\\n\\nThere are several approaches to integrating external knowledge with language models. One common method is to pretrain the language model on a large corpus of text that includes both general knowledge and domain-specific information. This helps the model learn to generate responses that are informed by external knowledge.\\n\\nAnother approach is to use knowledge graphs, which represent information as nodes and edges, to connect entities and their relationships. By incorporating a knowledge graph into a language model, it can understand and reason about the relationships between different entities, enabling it to provide more accurate and coherent responses.\\n\\nAdditionally, some researchers have explored techniques like knowledge distillation, where a language model is trained to mimic the behavior of another model that has access to external knowledge. This allows the model to learn from the knowledge-rich model and improve its performance.\\n\\nOverall, integrating external knowledge with Large Language Models has the potential to significantly enhance their capabilities and make them more useful in real-world applications. It's an exciting area of research with many possibilities for innovation and improvement. Is there anything specific you would like to know about this topic?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"My interest here is to explore the potential of integrating Large Language Models with external knowledge\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "3f2e98d9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f2e98d9",
        "outputId": "815833ee-c6f2-4b07-ab51-156d21a7750b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 694 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I just want to analyze the different possibilities. What can you think of?',\n",
              " 'history': \"Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge\\nAI: That's a fascinating topic! Integrating Large Language Models with external knowledge can greatly enhance their capabilities and make them more useful in various applications. By incorporating external knowledge sources, such as databases, knowledge graphs, or even the internet, language models can access a vast amount of information to provide more accurate and contextually relevant responses.\\n\\nThere are several approaches to integrating external knowledge with language models. One common method is to pretrain the language model on a large corpus of text that includes both general knowledge and domain-specific information. This helps the model learn to generate responses that are informed by external knowledge.\\n\\nAnother approach is to use knowledge graphs, which represent information as nodes and edges, to connect entities and their relationships. By incorporating a knowledge graph into a language model, it can understand and reason about the relationships between different entities, enabling it to provide more accurate and coherent responses.\\n\\nAdditionally, some researchers have explored techniques like knowledge distillation, where a language model is trained to mimic the behavior of another model that has access to external knowledge. This allows the model to learn from the knowledge-rich model and improve its performance.\\n\\nOverall, integrating external knowledge with Large Language Models has the potential to significantly enhance their capabilities and make them more useful in real-world applications. It's an exciting area of research with many possibilities for innovation and improvement. Is there anything specific you would like to know about this topic?\",\n",
              " 'response': 'There are several possibilities when it comes to integrating Large Language Models with external knowledge. Here are a few examples:\\n\\n1. Knowledge Graph Integration: As I mentioned earlier, incorporating a knowledge graph into a language model can enable it to understand and reason about relationships between entities. This can be particularly useful in tasks like question answering, where the model needs to retrieve information from a knowledge base to provide accurate responses.\\n\\n2. Database Integration: Language models can also be integrated with databases to access structured information. This can be helpful in scenarios where the model needs to retrieve specific data or perform complex queries. For example, a language model integrated with a customer database can provide personalized responses based on individual customer information.\\n\\n3. Internet Integration: By connecting language models to the internet, they can access a vast amount of information available online. This can be useful for tasks like fact-checking, summarization, or generating contextually relevant responses based on real-time information.\\n\\n4. Domain-Specific Knowledge Integration: Language models can be trained on domain-specific data to specialize in a particular field. For example, a language model trained on medical literature can provide more accurate and detailed responses in the medical domain.\\n\\n5. Multi-modal Integration: Language models can also be integrated with other modalities, such as images or videos, to provide a more comprehensive understanding of the input. This can be useful in tasks like image captioning or video summarization.\\n\\nThese are just a few possibilities, and there are many other ways to integrate external knowledge with Large Language Models. The choice of integration method depends on the specific task and the available resources.'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"I just want to analyze the different possibilities. What can you think of?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a2a8d062",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2a8d062",
        "outputId": "c33f6bc6-c42a-408c-a5ae-88ad17704030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 739 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Which data source types could be used to give context to the model?',\n",
              " 'history': 'Human: I just want to analyze the different possibilities. What can you think of?\\nAI: There are several possibilities when it comes to integrating Large Language Models with external knowledge. Here are a few examples:\\n\\n1. Knowledge Graph Integration: As I mentioned earlier, incorporating a knowledge graph into a language model can enable it to understand and reason about relationships between entities. This can be particularly useful in tasks like question answering, where the model needs to retrieve information from a knowledge base to provide accurate responses.\\n\\n2. Database Integration: Language models can also be integrated with databases to access structured information. This can be helpful in scenarios where the model needs to retrieve specific data or perform complex queries. For example, a language model integrated with a customer database can provide personalized responses based on individual customer information.\\n\\n3. Internet Integration: By connecting language models to the internet, they can access a vast amount of information available online. This can be useful for tasks like fact-checking, summarization, or generating contextually relevant responses based on real-time information.\\n\\n4. Domain-Specific Knowledge Integration: Language models can be trained on domain-specific data to specialize in a particular field. For example, a language model trained on medical literature can provide more accurate and detailed responses in the medical domain.\\n\\n5. Multi-modal Integration: Language models can also be integrated with other modalities, such as images or videos, to provide a more comprehensive understanding of the input. This can be useful in tasks like image captioning or video summarization.\\n\\nThese are just a few possibilities, and there are many other ways to integrate external knowledge with Large Language Models. The choice of integration method depends on the specific task and the available resources.',\n",
              " 'response': \"There are various data source types that can be used to give context to a language model. Some examples include:\\n\\n1. Textual Data: Language models can be trained on large amounts of textual data, such as books, articles, or web pages. This helps the model learn grammar, vocabulary, and general knowledge about the world.\\n\\n2. Knowledge Bases: Knowledge bases, such as Wikipedia or Wikidata, can provide structured information about entities, their attributes, and relationships. Integrating a knowledge base with a language model allows it to access factual information and reason about it.\\n\\n3. Databases: Structured databases can be used to provide specific information or perform complex queries. For example, a customer database can give context about individual customers, their preferences, or purchase history.\\n\\n4. Internet Data: Language models can be connected to the internet to access real-time information. This can include news articles, social media posts, or any other publicly available online content.\\n\\n5. Domain-Specific Data: Data from specific domains, such as scientific literature, medical records, or legal documents, can be used to train language models specialized in those areas. This helps the model understand domain-specific terminology and context.\\n\\n6. Multi-modal Data: Language models can also be trained on multi-modal data, which includes text, images, videos, or audio. This allows the model to understand and generate responses based on multiple modalities.\\n\\nThese are just a few examples of data source types that can provide context to a language model. The choice of data source depends on the specific task and the type of information required to enhance the model's understanding.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"Which data source types could be used to give context to the model?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "ff199a3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff199a3f",
        "outputId": "b66a03ab-9b63-43bb-eac8-cb5d96d118fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 451 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my aim again?',\n",
              " 'history': \"Human: Which data source types could be used to give context to the model?\\nAI: There are various data source types that can be used to give context to a language model. Some examples include:\\n\\n1. Textual Data: Language models can be trained on large amounts of textual data, such as books, articles, or web pages. This helps the model learn grammar, vocabulary, and general knowledge about the world.\\n\\n2. Knowledge Bases: Knowledge bases, such as Wikipedia or Wikidata, can provide structured information about entities, their attributes, and relationships. Integrating a knowledge base with a language model allows it to access factual information and reason about it.\\n\\n3. Databases: Structured databases can be used to provide specific information or perform complex queries. For example, a customer database can give context about individual customers, their preferences, or purchase history.\\n\\n4. Internet Data: Language models can be connected to the internet to access real-time information. This can include news articles, social media posts, or any other publicly available online content.\\n\\n5. Domain-Specific Data: Data from specific domains, such as scientific literature, medical records, or legal documents, can be used to train language models specialized in those areas. This helps the model understand domain-specific terminology and context.\\n\\n6. Multi-modal Data: Language models can also be trained on multi-modal data, which includes text, images, videos, or audio. This allows the model to understand and generate responses based on multiple modalities.\\n\\nThese are just a few examples of data source types that can provide context to a language model. The choice of data source depends on the specific task and the type of information required to enhance the model's understanding.\",\n",
              " 'response': \"I'm sorry, but I don't have access to your personal information or previous conversations, so I don't know what your specific aim is. Could you please provide more context or clarify your question?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_bufw,\n",
        "    \"What is my aim again?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f59f77",
      "metadata": {
        "id": "f5f59f77"
      },
      "source": [
        "As we can see, it effectively 'fogot' what we talked about in the first interaction. Let's see what it 'remembers'. Given that we set k to be `1`, we would expect it remembers only the last interaction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b354c8d",
      "metadata": {
        "id": "6b354c8d"
      },
      "source": [
        "We need to access a special method here since, in this memory type, the buffer is first passed through this method to be sent later to the llm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "85266406",
      "metadata": {
        "id": "85266406"
      },
      "outputs": [],
      "source": [
        "bufw_history = conversation_bufw.memory.load_memory_variables(\n",
        "    inputs=[]\n",
        ")['history']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5904ae2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5904ae2a",
        "outputId": "bd0aa797-7a43-4af5-a531-209aa6272dd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is my aim again?\n",
            "AI:  Your aim is to use data sources to give context to the model.\n"
          ]
        }
      ],
      "source": [
        "print(bufw_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae8b937d",
      "metadata": {
        "id": "ae8b937d"
      },
      "source": [
        "Makes sense.\n",
        "\n",
        "On the plus side, we are shortening our conversation length when compared to buffer memory _without_ a window:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "9fbb50fe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fbb50fe",
        "outputId": "a2d2ae89-352f-4d11-da93-a270dfb5f2a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffer memory conversation length: 1009\n",
            "Summary memory conversation length: 133\n",
            "Buffer window memory conversation length: 50\n"
          ]
        }
      ],
      "source": [
        "print(\n",
        "    f'Buffer memory conversation length: {len(tokenizer.encode(conversation_buf.memory.buffer))}\\n'\n",
        "    f'Summary memory conversation length: {len(tokenizer.encode(conversation_sum.memory.buffer))}\\n'\n",
        "    f'Buffer window memory conversation length: {len(tokenizer.encode(bufw_history))}'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69842cc1",
      "metadata": {
        "id": "69842cc1"
      },
      "source": [
        "_Practical Note: We are using `k=2` here for illustrative purposes, in most real world applications you would need a higher value for k._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aea5fc8",
      "metadata": {
        "id": "2aea5fc8"
      },
      "source": [
        "### More memory types!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daeb5162",
      "metadata": {
        "id": "daeb5162"
      },
      "source": [
        "Given that we understand memory already, we will present a few more memory types here and hopefully a brief description will be enough to understand their underlying functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0365333",
      "metadata": {
        "id": "f0365333"
      },
      "source": [
        "#### ConversationSummaryBufferMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317f298e",
      "metadata": {
        "id": "317f298e"
      },
      "source": [
        "**Key feature:** _the conversation summary memory keeps a summary of the earliest pieces of conversation while retaining a raw recollection of the latest interactions._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57ef5c8b",
      "metadata": {
        "id": "57ef5c8b"
      },
      "source": [
        "#### ConversationKnowledgeGraphMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40248f03",
      "metadata": {
        "id": "40248f03"
      },
      "source": [
        "This is a super cool memory type that was introduced just [recently](https://twitter.com/LangChainAI/status/1625158388824043522). It is based on the concept of a _knowledge graph_ which recognizes different entities and connects them in pairs with a predicate resulting in (subject, predicate, object) triplets. This enables us to compress a lot of information into highly significant snippets that can be fed into the model as context. If you want to understand this memory type in more depth you can check out [this](https://apex974.com/articles/explore-langchain-support-for-knowledge-graph) blogpost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91952cd1",
      "metadata": {
        "id": "91952cd1"
      },
      "source": [
        "**Key feature:** _the conversation knowledge graph memory keeps a knowledge graph of all the entities that have been mentioned in the interactions together with their semantic relationships._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02241bc3",
      "metadata": {
        "id": "02241bc3"
      },
      "outputs": [],
      "source": [
        "# you may need to install this library\n",
        "# !pip install -qU networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "c5f10a89",
      "metadata": {
        "id": "c5f10a89"
      },
      "outputs": [],
      "source": [
        "conversation_kg = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=ConversationKGMemory(llm=llm)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "65957fe2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65957fe2",
        "outputId": "498b27d8-3e87-436a-e8ce-140586cc1782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spent a total of 1228 tokens\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'My name is human and I like mangoes!',\n",
              " 'history': '',\n",
              " 'response': \"Hello, Human! It's nice to meet you. I'm an AI and I'm here to chat with you. Mangoes are delicious! They are tropical fruits that belong to the Anacardiaceae family. They are native to South Asia but are now grown in many parts of the world. Mangoes are known for their sweet and juicy flesh, which can range in color from yellow to orange. They are also a good source of vitamins A and C, as well as dietary fiber. Do you have a favorite variety of mango?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "count_tokens(\n",
        "    conversation_kg,\n",
        "    \"My name is human and I like mangoes!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74054534",
      "metadata": {
        "id": "74054534"
      },
      "source": [
        "The memory keeps a knowledge graph of everything it learned so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "5a8c54fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a8c54fb",
        "outputId": "8bbdc7bb-3afb-4826-de2c-a584584f595e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Human', 'human', 'name is'), ('Human', 'mangoes', 'likes')]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "conversation_kg.memory.kg.get_triples()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1a1ca15",
      "metadata": {
        "id": "e1a1ca15"
      },
      "source": [
        "#### ConversationEntityMemory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e9aeaf",
      "metadata": {
        "id": "41e9aeaf"
      },
      "source": [
        "**Key feature:** _the conversation entity memory keeps a recollection of the main entities that have been mentioned, together with their specific attributes._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2900a385",
      "metadata": {
        "id": "2900a385"
      },
      "source": [
        "The way this works is quite similar to the `ConversationKnowledgeGraphMemory`, you can refer to the [docs](https://python.langchain.com/en/latest/modules/memory/types/entity_summary_memory.html) if you want to see it in action."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d45112bd",
      "metadata": {
        "id": "d45112bd"
      },
      "source": [
        "## What else can we do with memory?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78296bff",
      "metadata": {
        "id": "78296bff"
      },
      "source": [
        "There are several cool things we can do with memory in langchain. We can:\n",
        "* implement our own custom memory module\n",
        "* use multiple memory modules in the same chain\n",
        "* combine agents with memory and other tools\n",
        "\n",
        "If this piques your interest, we suggest you to go take a look at the memory [how-to](https://langchain.readthedocs.io/en/latest/modules/memory/how_to_guides.html) section in the docs!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}