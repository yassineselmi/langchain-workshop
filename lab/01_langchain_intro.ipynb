{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yassineselmi/langchain-workshop/blob/main/lab/01_langchain_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWGzucuFfbBn"
      },
      "source": [
        "# Intro to LangChain\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around **L**arge **L**anguage **M**odels. It can be used to for chatbots, **G**enerative **Q**uestion-**A**nwering (GQA), summarization, and much more.\n",
        "\n",
        "The core idea of the library is that we can _\"chain\"_ together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
        "\n",
        "* **Prompt templates**: Prompt templates are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "* **LLMs**: Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "* **Agents**: Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
        "\n",
        "* **Memory**: Short-term memory, long-term memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-ryCeG_f_GC"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNaXrEPOhbuL"
      },
      "source": [
        "# Using LLMs in LangChain\n",
        "\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "## Hugging Face\n",
        "\n",
        "We first need to install additional prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LWA15ZkVjg80"
      },
      "outputs": [],
      "source": [
        "!pip install -qU huggingface_hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-whfR5Tjf1O"
      },
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sRGTytxCjKaW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exAl3iQgnAra"
      },
      "source": [
        "We can then generate text using a HF Hub model (we'll use `google/flan-t5-large`) using the Inference API built into Hugging Face Hub.\n",
        "\n",
        "_(The default Inference API doesn't use specialized hardware and so can be slow and cannot run larger models like `bigscience/bloom-560m` or `google/flan-t5-xxl`)_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7yubiSJhIfs"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "from langchain import HuggingFaceTextGenInference\n",
        "\n",
        "# initialize HF LLM\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\",\n",
        "    model_kwargs={\"temperature\":1, \"max_length\": 4096}\n",
        ")\n",
        "\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "question = \"Write a poem about open-source projects.\"\n",
        "answer = llm_chain.run(question)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpdXG9YtzrLJ"
      },
      "source": [
        "## OpenAI\n",
        "\n",
        "Start by installing additional prerequisites:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "FHo2YRHPDgHH"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fOo9qQvDgkz"
      },
      "source": [
        "We can also use OpenAI's generative models. The process is similar, we need to\n",
        "give our API key which can be retrieved by signing up for an account on the\n",
        "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deWmOJecfbBr"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "OPENAI_API_KEY = getpass(\"OpenAI API key: \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AWnaTCP0Ryg"
      },
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with `gpt-3.5-turbo`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZhQSDoYe0ly4"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "gpt35_llm = ChatOpenAI(\n",
        "    openai_api_key=OPENAI_API_KEY,\n",
        "    temperature=0,\n",
        "    model='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGL2zs3uEVj6"
      },
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM `gpt35_llm`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVSsC3iGEPAp",
        "outputId": "be90c765-2819-4dc6-8bf3-b72ee15f0bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the realm of code, where dreams take flight,\n",
            "Lies a world of wonders, shining so bright.\n",
            "Open-source projects, a symphony of minds,\n",
            "Unleashing creativity, where innovation finds.\n",
            "\n",
            "Like a tapestry woven with threads of care,\n",
            "Collaboration dances, a cosmic affair.\n",
            "From the depths of knowledge, ideas arise,\n",
            "Shared with the world, a priceless prize.\n",
            "\n",
            "In this digital realm, where boundaries dissolve,\n",
            "A community thrives, problems they resolve.\n",
            "A chorus of developers, diverse and vast,\n",
            "Uniting their skills, the future they cast.\n",
            "\n",
            "From the depths of the code, a vision takes shape,\n",
            "A masterpiece born, with no strings to drape.\n",
            "For open-source projects, they know no bounds,\n",
            "Empowering the world, with their profound sounds.\n",
            "\n",
            "Like a phoenix rising, from ashes of old,\n",
            "Open-source projects, their stories unfold.\n",
            "They break down barriers, they bridge the divide,\n",
            "A beacon of hope, where possibilities reside.\n",
            "\n",
            "With transparency as their guiding light,\n",
            "Open-source projects, they shine so bright.\n",
            "They foster inclusion, embrace the unknown,\n",
            "A sanctuary for ideas, where brilliance is sown.\n",
            "\n",
            "From the smallest bug fix to grand innovations,\n",
            "Open-source projects, they fuel aspirations.\n",
            "They empower the curious, the dreamers, the bold,\n",
            "To shape a future, where wonders unfold.\n",
            "\n",
            "So let us celebrate this digital art,\n",
            "Where collaboration ignites, right from the start.\n",
            "For open-source projects, they hold the key,\n",
            "To a world where knowledge is forever free.\n"
          ]
        }
      ],
      "source": [
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=gpt35_llm\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMkI18xfbBr"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12 (main, Apr  5 2022, 01:52:34) \n[Clang 12.0.0 ]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}